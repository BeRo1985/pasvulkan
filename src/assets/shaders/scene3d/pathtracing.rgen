#version 460 core

precision highp float;
precision highp int;

#extension GL_EXT_nonuniform_qualifier : require
#extension GL_GOOGLE_include_directive : require
#extension GL_EXT_ray_tracing : require
#extension GL_EXT_buffer_reference : require
#extension GL_EXT_multiview : require

#define RAYTRACING
#define USE_MATERIAL_BUFFER_REFERENCE
#define USE_BUFFER_REFERENCE
#define LIGHTS

#include "globaldescriptorset.glsl"

struct View {
  mat4 viewMatrix;
  mat4 projectionMatrix;
  mat4 inverseViewMatrix;
  mat4 inverseProjectionMatrix;
};

layout(set = 1, binding = 0, std140) uniform uboViews {
  View views[256]; // 65536 / (64 * 4) = 256
} uView;

layout(set = 1, binding = 1, rgba32f) uniform image2DArray uImageAccumulated;
layout(set = 1, binding = 2, r32ui) uniform writeonly uimage2DArray uImageOutput;

#define PATHTRACING_PUSHCONSTANTS
#include "pathtracing.glsl"

#include "hash.glsl"

#include "floatint.glsl"

#include "rgb9e5.glsl"

layout(location = 0) rayPayloadEXT RayPayload payload;

bool ProjectionMatrixIsReversedZ(const in mat4 projectionMatrix){
  return projectionMatrix[2][3] < -1e-7;
}

bool ProjectionMatrixIsInfiniteFarPlane(const in mat4 projectionMatrix){
  return ProjectionMatrixIsReversedZ(projectionMatrix) && ((abs(projectionMatrix[2][2]) < 1e-7) && (abs(projectionMatrix[3][2]) > 1e-7));
}

float GetZFarDepthValue(const in mat4 projectionMatrix){
  return ProjectionMatrixIsReversedZ(projectionMatrix) ? 0.0 : 1.0;
}

void GetCameraPositionDirection(out vec3 origin, 
                                out vec3 direction,
                                const in View view,
                                const in vec2 uv){ 

#if 0

  // We need to compute the origin and direction of the primary ray in the more safe way, for just to be sure.

  bool reversedZ = ProjectionMatrixIsReversedZ(view.projectionMatrix);

  mat4 inverseViewProjectionMatrix = view.inverseViewMatrix * view.inverseProjectionMatrix;

  vec4 nearPlane = inverseViewProjectionMatrix * vec4(fma(uv, vec2(2.0), vec2(-1.0)), reversedZ ? 1.0 : 0.0, 1.0);
  nearPlane /= nearPlane.w;

  vec4 farPlane = inverseViewProjectionMatrix * vec4(fma(uv, vec2(2.0), vec2(-1.0)), reversedZ ? 0.0 : 1.0, 1.0);
  farPlane /= farPlane.w;

  origin = nearPlane.xyz;
  direction = normalize(farPlane.xyz - nearPlane.xyz);

#else

  // For the main rendering, we can use a faster way to compute the origin and direction of the primary ray.

  bool reversedZ = ProjectionMatrixIsReversedZ(view.projectionMatrix);

  vec4 nearPlane = vec4(fma(uv, vec2(2.0), vec2(-1.0)), reversedZ ? 1.0 : 0.0, 1.0);

  vec4 cameraDirection = vec4((view.inverseProjectionMatrix * nearPlane).xyz, 0.0); 

#if 0    
  
  // Works also for orthographic projection (and for all projection types)
  
  vec4 primaryRayOrigin = view.inverseViewProjectionMatrix * nearPlane;
  primaryRayOrigin /= primaryRayOrigin.w;

  origin = primaryRayOrigin.xyz;

#else

  // Works only for perspective projection, not for orthographic projection, but is faster

  origin = view.inverseViewMatrix[3].xyz;

#endif

  direction = normalize((view.inverseViewMatrix * cameraDirection).xyz);

#endif

}

void main(){

  uvec3 launchID = gl_LaunchIDEXT;
  
  uint viewIndex = pushConstants.viewBaseIndex + uint(launchID.z);

  View view = uView.views[viewIndex];

  uint samplesPerPixels = pushConstants.samplesPerPixelsMaxDepth.x;
  uint maxDepth = pushConstants.samplesPerPixelsMaxDepth.y;

//uvec4 seed = hash44ChaCha20(uvec4(launchID.xy, pushConstants.frameIndex, 0u));

  vec3 radiance = vec3(0.0);

  float tMin = uintBitsToFloat(0x00000001u); // Minimum positive subnormal value
  float tMax = uintBitsToFloat(0x7F7FFFFFu); // Maximum positive normal value
  
  for(uint sampleIndex = 0; sampleIndex < samplesPerPixels; sampleIndex++){

    uvec4 randomUInt = hash44ChaCha20(uvec4(launchID.xy, pushConstants.frameIndex, sampleIndex)); 

    vec4 randomFloat = makeAbsFloat(randomUInt);

    vec2 jitter = fma(randomFloat.xy, vec2(1.0), vec2(-0.5));

    vec2 uv = (vec2(launchID.xy) + jitter) / vec2(pushConstants.timeSecondsTimeFractionalSecondWidthHeight.zw);

    vec3 origin, direction;
    GetCameraPositionDirection(origin, direction, view, uv);

    Ray ray = Ray(origin, direction);

		vec3 beta = vec3(1.0);
		vec3 absorption = vec3(0.0);
		vec3 pathRadiance = vec3(0.0);
		BsdfSample bsdf;
    
    for(uint depthIndex = 0; depthIndex < maxDepth; depthIndex++){
      
      payload.depth = depthIndex;
			payload.stop = false;
			payload.radiance = pathRadiance;
			payload.beta = beta;
			payload.ray = ray;
			payload.bsdf = bsdf;
			payload.absorption = absorption;

      traceRayEXT(
        uRaytracingTopLevelAccelerationStructure, // acceleration structure
        gl_RayFlagsOpaqueEXT,                     // rayFlags
        0xff,                                     // cullMask
        0,                                        // sbtRecordOffset
        0,                                        // sbtRecordStride
        0,                                        // missIndex
        ray.origin,                               // ray origin
        tMin,                                     // ray min range
        ray.direction,                            // ray direction
        tMax,                                     // ray max range
        0                                         // payload (location = 0)
      );

      pathRadiance = payload.radiance;
      beta = payload.beta;      
      ray = payload.ray;
      bsdf = payload.bsdf;
      absorption = payload.absorption;

      if(depthIndex == 0u){
        // TODO: Store information about the first intersection for denoising
      }

      if(payload.stop){
        break;
      }

    }

    radiance += pathRadiance;

  } 

  radiance /= float(samplesPerPixels);

  vec4 accumlated = vec4((pushConstants.frameIndex == 0u) ? vec3(0.0) : vec3(imageLoad(uImageAccumulated, ivec3(launchID.xyz)).xyz) + radiance, 1.0);

  imageStore(uImageAccumulated, ivec3(launchID.xyz), accumlated);

  imageStore(uImageOutput, ivec3(launchID.xyz), uvec4(encodeRGB9E5(accumlated.xyz)));

}